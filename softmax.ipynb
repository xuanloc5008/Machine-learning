{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([256, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random_seed = 123\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "num_features = 784\n",
    "num_classes = 10\n",
    "\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n",
    "\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape) \n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(y, num_classes):\n",
    "    y_onehot = torch.FloatTensor(y.size(0), num_classes)\n",
    "    y_onehot.zero_()\n",
    "    y_cpu = y.to(torch.device('cpu'))\n",
    "    y_onehot.scatter_(1, y_cpu.view(-1, 1).long(), 1).float()\n",
    "    return y_onehot.to(DEVICE)\n",
    "\n",
    "def softmax(z):\n",
    "    return (torch.exp(z.t()) / torch.sum(torch.exp(z), dim=1)).t()\n",
    "                    \n",
    "def cross_entropy(softmax, y_target):\n",
    "    return -torch.sum(torch.log(softmax) * (y_target), dim=1)\n",
    "\n",
    "class SoftmaxRegression1():\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.weights = torch.zeros(num_classes, num_features, # h x m\n",
    "                                   dtype=torch.float32, device=DEVICE)\n",
    "        self.bias = torch.zeros(num_classes, dtype=torch.float32, device=DEVICE) # h\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = torch.mm(x, self.weights.t()) + self.bias # net inputs\n",
    "        probas = softmax(logits) # activations\n",
    "        return logits, probas\n",
    "        \n",
    "    def backward(self, x, y, probas):  \n",
    "        grad_loss_wrt_w = -torch.mm(x.t(), y - probas).t()\n",
    "        grad_loss_wrt_b = -torch.sum(y - probas)\n",
    "        return grad_loss_wrt_w, grad_loss_wrt_b\n",
    "            \n",
    "    def predict_labels(self, x):\n",
    "        logits, probas = self.forward(x)\n",
    "        labels = torch.argmax(probas, dim=1)\n",
    "        return labels    \n",
    "            \n",
    "    def evaluate(self, x, y):\n",
    "        labels = self.predict_labels(x).float()\n",
    "        accuracy = torch.sum(labels.view(-1) == y.float()).item() / y.size(0)\n",
    "        return accuracy\n",
    "    \n",
    "    def train(self, x, y, num_epochs, learning_rate=0.01):\n",
    "        epoch_cost = []\n",
    "        for e in range(num_epochs):\n",
    "            \n",
    "            y_onehot = to_onehot(y, num_classes=self.num_classes)\n",
    "            \n",
    "            logits, probas = self.backward(x)\n",
    "            \n",
    "            grad_w, grad_b = self.backward(x, y_onehot, probas)\n",
    "\n",
    "            self.weights -= learning_rate * grad_w / y.size(0)\n",
    "            self.bias -= learning_rate * grad_b / y.size(0)\n",
    "            \n",
    "            logits, probas = self.forward(x)\n",
    "            cost = torch.mean(cross_entropy(probas, y_onehot))\n",
    "            print('Epoch: %03d' % (e+1), end=\"\")\n",
    "            print(' | Train ACC: %.3f' % self.evaluate(x, y), end=\"\")\n",
    "            print(' | Cost: %.3f' % cost)\n",
    "            epoch_cost.append(cost)\n",
    "        return epoch_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Train ACC: 0.578 | Cost: 2.179\n",
      "Epoch: 001 | Train ACC: 0.602 | Cost: 2.077\n",
      "Epoch: 001 | Train ACC: 0.633 | Cost: 1.998\n",
      "Epoch: 001 | Train ACC: 0.746 | Cost: 1.915\n",
      "Epoch: 001 | Train ACC: 0.684 | Cost: 1.843\n",
      "Epoch: 001 | Train ACC: 0.719 | Cost: 1.779\n",
      "Epoch: 001 | Train ACC: 0.754 | Cost: 1.732\n",
      "Epoch: 001 | Train ACC: 0.730 | Cost: 1.658\n",
      "Epoch: 001 | Train ACC: 0.777 | Cost: 1.598\n",
      "Epoch: 001 | Train ACC: 0.785 | Cost: 1.522\n",
      "Epoch: 001 | Train ACC: 0.816 | Cost: 1.496\n",
      "Epoch: 001 | Train ACC: 0.797 | Cost: 1.461\n",
      "Epoch: 001 | Train ACC: 0.809 | Cost: 1.418\n",
      "Epoch: 001 | Train ACC: 0.828 | Cost: 1.335\n",
      "Epoch: 001 | Train ACC: 0.820 | Cost: 1.335\n",
      "Epoch: 001 | Train ACC: 0.828 | Cost: 1.267\n",
      "Epoch: 001 | Train ACC: 0.797 | Cost: 1.234\n",
      "Epoch: 001 | Train ACC: 0.797 | Cost: 1.233\n",
      "Epoch: 001 | Train ACC: 0.809 | Cost: 1.196\n",
      "Epoch: 001 | Train ACC: 0.824 | Cost: 1.113\n",
      "Epoch: 001 | Train ACC: 0.809 | Cost: 1.209\n",
      "Epoch: 001 | Train ACC: 0.812 | Cost: 1.131\n",
      "Epoch: 001 | Train ACC: 0.805 | Cost: 1.106\n",
      "Epoch: 001 | Train ACC: 0.801 | Cost: 1.097\n",
      "Epoch: 001 | Train ACC: 0.809 | Cost: 1.069\n",
      "Epoch: 001 | Train ACC: 0.836 | Cost: 1.085\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 1.005\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.965\n",
      "Epoch: 001 | Train ACC: 0.793 | Cost: 1.027\n",
      "Epoch: 001 | Train ACC: 0.812 | Cost: 1.025\n",
      "Epoch: 001 | Train ACC: 0.820 | Cost: 0.973\n",
      "Epoch: 001 | Train ACC: 0.777 | Cost: 0.989\n",
      "Epoch: 001 | Train ACC: 0.824 | Cost: 0.936\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.938\n",
      "Epoch: 001 | Train ACC: 0.805 | Cost: 0.986\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.887\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.876\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.891\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.873\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.849\n",
      "Epoch: 001 | Train ACC: 0.777 | Cost: 0.934\n",
      "Epoch: 001 | Train ACC: 0.816 | Cost: 0.855\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.822\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.856\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.820\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.756\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.779\n",
      "Epoch: 001 | Train ACC: 0.816 | Cost: 0.794\n",
      "Epoch: 001 | Train ACC: 0.828 | Cost: 0.789\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.780\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.744\n",
      "Epoch: 001 | Train ACC: 0.816 | Cost: 0.794\n",
      "Epoch: 001 | Train ACC: 0.848 | Cost: 0.756\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.706\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.801\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.693\n",
      "Epoch: 001 | Train ACC: 0.848 | Cost: 0.777\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.716\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.694\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.742\n",
      "Epoch: 001 | Train ACC: 0.883 | Cost: 0.654\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.703\n",
      "Epoch: 001 | Train ACC: 0.824 | Cost: 0.680\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.673\n",
      "Epoch: 001 | Train ACC: 0.836 | Cost: 0.721\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.678\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.640\n",
      "Epoch: 001 | Train ACC: 0.848 | Cost: 0.717\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.661\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.641\n",
      "Epoch: 001 | Train ACC: 0.820 | Cost: 0.709\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.651\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.645\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.678\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.639\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.665\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.669\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.678\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.642\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.592\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.711\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.684\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.670\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.619\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.660\n",
      "Epoch: 001 | Train ACC: 0.879 | Cost: 0.621\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.674\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.687\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.644\n",
      "Epoch: 001 | Train ACC: 0.879 | Cost: 0.557\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.582\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.662\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.670\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.624\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.656\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.622\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.674\n",
      "Epoch: 001 | Train ACC: 0.848 | Cost: 0.652\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.641\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.644\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.585\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.545\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.640\n",
      "Epoch: 001 | Train ACC: 0.816 | Cost: 0.697\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.602\n",
      "Epoch: 001 | Train ACC: 0.848 | Cost: 0.618\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.579\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.581\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.580\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.560\n",
      "Epoch: 001 | Train ACC: 0.883 | Cost: 0.581\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.632\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.604\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.579\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.546\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.572\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.566\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.552\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.554\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.541\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.554\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.637\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.486\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.552\n",
      "Epoch: 001 | Train ACC: 0.883 | Cost: 0.534\n",
      "Epoch: 001 | Train ACC: 0.895 | Cost: 0.501\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.602\n",
      "Epoch: 001 | Train ACC: 0.914 | Cost: 0.450\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.517\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.571\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.633\n",
      "Epoch: 001 | Train ACC: 0.848 | Cost: 0.569\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.590\n",
      "Epoch: 001 | Train ACC: 0.828 | Cost: 0.556\n",
      "Epoch: 001 | Train ACC: 0.883 | Cost: 0.555\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.533\n",
      "Epoch: 001 | Train ACC: 0.898 | Cost: 0.488\n",
      "Epoch: 001 | Train ACC: 0.898 | Cost: 0.510\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.572\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.524\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.543\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.588\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.534\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.614\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.556\n",
      "Epoch: 001 | Train ACC: 0.910 | Cost: 0.491\n",
      "Epoch: 001 | Train ACC: 0.895 | Cost: 0.502\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.543\n",
      "Epoch: 001 | Train ACC: 0.898 | Cost: 0.538\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.540\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.449\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.575\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.540\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.518\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.574\n",
      "Epoch: 001 | Train ACC: 0.812 | Cost: 0.612\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.506\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.459\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.564\n",
      "Epoch: 001 | Train ACC: 0.906 | Cost: 0.430\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.544\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.558\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.558\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.490\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.500\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.485\n",
      "Epoch: 001 | Train ACC: 0.902 | Cost: 0.489\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.568\n",
      "Epoch: 001 | Train ACC: 0.883 | Cost: 0.501\n",
      "Epoch: 001 | Train ACC: 0.898 | Cost: 0.445\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.544\n",
      "Epoch: 001 | Train ACC: 0.902 | Cost: 0.408\n",
      "Epoch: 001 | Train ACC: 0.910 | Cost: 0.478\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.537\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.591\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.510\n",
      "Epoch: 001 | Train ACC: 0.910 | Cost: 0.443\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.500\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.549\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.488\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.499\n",
      "Epoch: 001 | Train ACC: 0.879 | Cost: 0.495\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.524\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.476\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.505\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.570\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.519\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.487\n",
      "Epoch: 001 | Train ACC: 0.898 | Cost: 0.487\n",
      "Epoch: 001 | Train ACC: 0.906 | Cost: 0.459\n",
      "Epoch: 001 | Train ACC: 0.902 | Cost: 0.468\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.487\n",
      "Epoch: 001 | Train ACC: 0.871 | Cost: 0.532\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.535\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.542\n",
      "Epoch: 001 | Train ACC: 0.844 | Cost: 0.532\n",
      "Epoch: 001 | Train ACC: 0.832 | Cost: 0.580\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.533\n",
      "Epoch: 001 | Train ACC: 0.895 | Cost: 0.465\n",
      "Epoch: 001 | Train ACC: 0.926 | Cost: 0.392\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.482\n",
      "Epoch: 001 | Train ACC: 0.918 | Cost: 0.386\n",
      "Epoch: 001 | Train ACC: 0.883 | Cost: 0.463\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.548\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.544\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.459\n",
      "Epoch: 001 | Train ACC: 0.824 | Cost: 0.554\n",
      "Epoch: 001 | Train ACC: 0.898 | Cost: 0.394\n",
      "Epoch: 001 | Train ACC: 0.852 | Cost: 0.573\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.504\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.503\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.545\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.470\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.433\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.459\n",
      "Epoch: 001 | Train ACC: 0.914 | Cost: 0.390\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.541\n",
      "Epoch: 001 | Train ACC: 0.840 | Cost: 0.529\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.453\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.484\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.524\n",
      "Epoch: 001 | Train ACC: 0.863 | Cost: 0.491\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.550\n",
      "Epoch: 001 | Train ACC: 0.855 | Cost: 0.489\n",
      "Epoch: 001 | Train ACC: 0.867 | Cost: 0.471\n",
      "Epoch: 001 | Train ACC: 0.902 | Cost: 0.468\n",
      "Epoch: 001 | Train ACC: 0.895 | Cost: 0.435\n",
      "Epoch: 001 | Train ACC: 0.887 | Cost: 0.499\n",
      "Epoch: 001 | Train ACC: 0.859 | Cost: 0.515\n",
      "Epoch: 001 | Train ACC: 0.895 | Cost: 0.436\n",
      "Epoch: 001 | Train ACC: 0.891 | Cost: 0.421\n",
      "Epoch: 001 | Train ACC: 0.879 | Cost: 0.453\n",
      "Epoch: 001 | Train ACC: 0.902 | Cost: 0.402\n",
      "Epoch: 001 | Train ACC: 0.902 | Cost: 0.464\n",
      "Epoch: 001 | Train ACC: 0.875 | Cost: 0.424\n",
      "\n",
      "Model parameters:\n",
      "  Weights: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "  Bias: tensor([-2.3572e-08, -2.3572e-08, -2.3572e-08, -2.3572e-08, -2.3572e-08,\n",
      "        -2.3572e-08, -2.3572e-08, -2.3572e-08, -2.3572e-08, -2.3572e-08])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1 = SoftmaxRegression1(num_features=num_features, num_classes=num_classes)\n",
    "for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.view(-1, 28*28).to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        # Plotting the images and their labels\n",
    "        # fig, ax = plt.subplots(1, 4, figsize=(15, 4))\n",
    "        # for i in range(4):\n",
    "        #     ax[i].imshow(features[i].view(28, 28).cpu(), cmap='gray')\n",
    "        #     ax[i].set_title(f'Label: {targets[i].item()}')\n",
    "        #     ax[i].axis('off')\n",
    "        # plt.show()\n",
    "        # Train on the current batch\n",
    "        model1.train(features, targets, num_epochs=1, learning_rate=learning_rate)\n",
    "\n",
    "print('\\nModel parameters:')\n",
    "print('  Weights: %s' % model1.weights)\n",
    "print('  Bias: %s' % model1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "batch_num =50\n",
    "for i, (features, targets) in enumerate(test_loader):\n",
    "    if i == batch_num:\n",
    "        break\n",
    "    \n",
    "fig, ax = plt.subplots(1, 4)\n",
    "for i in range(4):\n",
    "    ax[i].imshow(features[i].view(28, 28), cmap=matplotlib.cm.binary)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels tensor([1, 7, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "_, predictions = model1.forward(features[:4].view(-1, 28*28).to(DEVICE))\n",
    "predictions = torch.argmax(predictions, dim=1)\n",
    "print('Predicted labels', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
